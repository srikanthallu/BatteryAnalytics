{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "try:\n",
    "    import batteryanalytics\n",
    "except ModuleNotFoundError as ie:\n",
    "    sys.path.append( os.path.join(os.path.abspath(\"\"), \"../\") )\n",
    "    import batteryanalytics\n",
    "from batteryanalytics import utils as ba_utils\n",
    "from batteryanalytics.utils import CNNModelKey as ModelKey\n",
    "\n",
    "from IPython.core.display import display, HTML, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basename = \"mechanical_loading_data.csv.gz\"\n",
    "dirname = \"../data\"\n",
    "\n",
    "target = \"Temperature [C]\"\n",
    "\n",
    "results_dirname = \"../cnn_results/cnn_models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_lookup(models):\n",
    "    best_model_lookup = dict()\n",
    "    for model_key,model in models.items():\n",
    "        fields_dict = model_key._asdict()\n",
    "        fields_dict[\"model_id\"] = \"best\"\n",
    "        best_model_key = ModelKey(**fields_dict)\n",
    "        res_model_key = best_model_lookup.setdefault(best_model_key, model_key)\n",
    "        if models[model_key].validate_loss[-1] < models[res_model_key].validate_loss[-1]:\n",
    "            best_model_lookup[best_model_key] = model_key\n",
    "    return best_model_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_performance(samples, target, predictions):\n",
    "    expected_performance_df = dict()\n",
    "    for model_key,prediction_mapping in predictions.items():\n",
    "        holdouts = model_key.holdouts\n",
    "        fields = [\n",
    "            field\n",
    "            for field in model_key._fields\n",
    "            if field not in [\"holdouts\", \"model_id\"]\n",
    "        ]\n",
    "        assert len(holdouts) == 1, f\"expecting 1 holdout but found {len(holdouts)}\"\n",
    "\n",
    "        offset = model_key.window + model_key.horizon - 1\n",
    "        y_true = samples[holdouts[0]].loc[:,pd.IndexSlice[:,:,target]].values.squeeze()\n",
    "        y_pred = prediction_mapping[holdouts[0]]\n",
    "        \n",
    "        expected_performance_df.setdefault(\n",
    "            tuple(getattr(model_key, field) for field in fields),\n",
    "            list()\n",
    "        ).extend(\n",
    "            np.square(y_true[offset:] - y_pred[offset:]).tolist()\n",
    "        )\n",
    "\n",
    "    expected_performance_df = pd.DataFrame(\n",
    "        [\n",
    "            key + (len(value), np.min(value), np.max(value), np.mean(value), np.std(value), np.median(value))\n",
    "            for key,value in expected_performance_df.items()\n",
    "        ],\n",
    "        columns=fields + [\"count\", \"min\", \"max\", \"mean\", \"std\", \"median\"]\n",
    "    )\n",
    "    expected_performance_df.sort_values(\n",
    "        by=list(expected_performance_df.columns), inplace=True\n",
    "    )\n",
    "    expected_performance_df.reset_index(\n",
    "        drop=True, inplace=True\n",
    "    )\n",
    "    return expected_performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_groupings(model_keys, grouping_fields):\n",
    "    grouping_dict = dict()\n",
    "    for model_key in model_keys:\n",
    "        fields_dict = model_key._asdict()\n",
    "        for field in grouping_fields:\n",
    "            fields_dict[field] = \"N/A\"\n",
    "        general_model_key = ModelKey(**fields_dict)\n",
    "        grouping_dict.setdefault(general_model_key, list()).append(model_key)\n",
    "    return grouping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_predictions(sample, sample_df, predictions, model_keys, model_labels=None):\n",
    "    if model_labels is None:\n",
    "        model_labels = list(map(str,model_keys))\n",
    "    assert len(model_keys) == len(model_labels), f\"length of model_keys ({len(model_keys)}) and model_labels ({len(model_labels)}) does not match\"\n",
    "    for model_key,model_label in zip(model_keys,model_labels):\n",
    "        values = predictions[model_key][sample]\n",
    "        values = np.hstack([\n",
    "            [np.nan]*(len(sample_df.index)-len(values)),\n",
    "            values\n",
    "        ])\n",
    "        sample_df[model_label] = values\n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model_groupings(sample, sample_df, target, predictions, best_model_lookup, grouping_fields, filename=None):\n",
    "    model_groupings = get_model_groupings(best_model_lookup.keys(), grouping_fields)\n",
    "    model_groupings = { \n",
    "        model_key:list(map(best_model_lookup.get,model_grouping))\n",
    "        for model_key,model_grouping in model_groupings.items()\n",
    "        if model_key.holdouts[0] == sample\n",
    "    }   \n",
    "    \n",
    "    n_cols = 3 \n",
    "    n_rows = int(np.ceil(len(model_groupings)/n_cols))\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(8*n_cols,5*n_rows))\n",
    "#     fig.set_constrained_layout_pads(w_pad=1.5)\n",
    "    gs = mpl.gridspec.GridSpec(\n",
    "        nrows=n_rows,\n",
    "        ncols=n_cols,\n",
    "        figure=fig\n",
    "    )   \n",
    "    \n",
    "    for ii,(general_model_key,model_group) in enumerate(sorted(model_groupings.items())):\n",
    "        title = \"\\n\".join(\n",
    "            f\"{field}={getattr(general_model_key,field)}\"\n",
    "            for field in ModelKey._fields\n",
    "            if field not in [\"holdouts\", \"model_id\"] + list(grouping_fields)\n",
    "        )   \n",
    "    \n",
    "        ax = fig.add_subplot(gs[ii])\n",
    "        ax.plot(sample_df.loc[:,pd.IndexSlice[:,:,target]], color=\"black\", label=target)\n",
    "        tmp_sample_df = input_predictions(\n",
    "            sample,\n",
    "            pd.DataFrame(index=sample_df.index),\n",
    "            predictions,\n",
    "            model_group,\n",
    "            model_labels=[\n",
    "                \", \".join([\n",
    "                    str(getattr(specific_model_key, field))\n",
    "                    for field in ModelKey._fields\n",
    "                    if field in grouping_fields\n",
    "                ])  \n",
    "                for specific_model_key in model_group\n",
    "            ]   \n",
    "        )   \n",
    "        ba_utils.display_df_columns(tmp_sample_df, title=title, title_loc=\"left\", ax=ax)\n",
    "        cur_col = ii % n_cols\n",
    "        cur_row = int(ii / n_cols)\n",
    "        if cur_col == 1 and cur_row+1 == n_rows:\n",
    "            ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.07), ncol=1)\n",
    "#     handles, labels = ax.get_legend_handles_labels() \n",
    "#     fig.legend(handles, labels, loc=\"upper center\", bbox_to_anchor=(0.5, -0.07), ncol=1)\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join(dirname, basename)\n",
    "raw_df = pd.read_csv(filename, header=[0,1,2], index_col=0, compression=\"gzip\")\n",
    "raw_df.info(verbose=True)\n",
    "with pd.option_context(\"display.max_rows\", 10, \"display.max_columns\", None):\n",
    "    display(raw_df)\n",
    "    display(raw_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_df = raw_df.copy()\n",
    "\n",
    "levels_0 = list()\n",
    "levels_1 = list()\n",
    "for column in data_df.columns:\n",
    "    if column[0] not in levels_0:\n",
    "        levels_0.append(column[0])\n",
    "    if column[1] not in levels_1:\n",
    "        levels_1.append(column[1])\n",
    "\n",
    "samples = {\n",
    "    pair:data_df.xs(pair, axis=\"columns\", level=(0,1), drop_level=False).dropna()\n",
    "    for pair in itertools.product(levels_0, levels_1)\n",
    "}\n",
    "summary1_df = pd.DataFrame(\n",
    "    [[\n",
    "            sample_df.index[-1]-sample_df.index[0],\n",
    "            len(sample_df.index)\n",
    "    ] for sample_df in samples.values() ],\n",
    "    columns=[\"time span\", \"samples\"]\n",
    ").describe()\n",
    "\n",
    "for sample_df in samples.values():\n",
    "    series = sample_df.loc[:,pd.IndexSlice[:,:,target]].copy()\n",
    "    idx = np.squeeze(series.values).argmax()\n",
    "    sample_df.drop(sample_df.index[2*idx+1:], inplace=True)\n",
    "#     vmin, vmax = series.min(), series.max()\n",
    "#     delta = vmax - vmin\n",
    "#     idx = series > 0.05*delta + vmin\n",
    "#     cut_off = np.squeeze(idx.iloc[::-1].idxmax().values).item()\n",
    "#     sample_df.drop(sample_df.index[sample_df.index > cut_off], inplace=True)\n",
    "\n",
    "summary2_df = pd.DataFrame(\n",
    "    [[\n",
    "            sample_df.index[-1]-sample_df.index[0],\n",
    "            len(sample_df.index)\n",
    "    ] for sample_df in samples.values() ],\n",
    "    columns=[\"time span\", \"samples\"]\n",
    ").describe()\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", 10, \"display.max_columns\", None):\n",
    "    display(summary1_df)\n",
    "    display(summary2_df)\n",
    "    display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_unique(items):\n",
    "    seen = set()\n",
    "    unique = list()\n",
    "    for item in items:\n",
    "        if item not in seen:\n",
    "            unique.append(item)\n",
    "            seen.add(item)\n",
    "    return unique        \n",
    "level0 = get_unique(raw_df.columns.get_level_values(0))\n",
    "level1 = get_unique(raw_df.columns.get_level_values(1))\n",
    "level2 = get_unique(raw_df.columns.get_level_values(2))\n",
    "\n",
    "for c0 in level0:\n",
    "    display(HTML(f\"<h1>{c0}</h1>\"))\n",
    "    \n",
    "#     if not c0.startswith(\"500\"):\n",
    "#         continue\n",
    "    for c1 in level1:\n",
    "        display(HTML(f\"<h2>{c1}</h2>\"))\n",
    "#         if not c1.startswith(\"20\"):\n",
    "#             continue\n",
    "        sample_df = samples[(c0,c1)]\n",
    "        \n",
    "        n_cols = 3\n",
    "        n_rows = int(np.ceil(len(level2)/n_cols))\n",
    "#         fig = plt.figure(constrained_layout=True, figsize=(n_cols*8,5*n_rows))\n",
    "        fig = plt.figure(constrained_layout=True, figsize=(16,8))\n",
    "        gs = mpl.gridspec.GridSpec(\n",
    "            nrows=n_rows,\n",
    "            ncols=n_cols,\n",
    "            figure=fig\n",
    "        )\n",
    "        for ii,c2 in enumerate(level2):\n",
    "            ax = fig.add_subplot(gs[ii])\n",
    "            ax.plot(sample_df[(c0,c1,c2)], zorder=1)\n",
    "            ax.plot(raw_df[(c0,c1,c2)], zorder=0)\n",
    "            ax.set_xlim(raw_df.index[0], raw_df.index[-1])\n",
    "            ax.set_title(c2)\n",
    "            ax.set_xlabel(\"Time\")\n",
    "        fig.suptitle(\"{}, {}\".format(c0,c1))\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_mapping = dict()\n",
    "for feature in set(data_df.columns.get_level_values(2)):\n",
    "    feature_mapping[ba_utils.sanitize_feature_name(feature)] = feature\n",
    "    \n",
    "time_function = ba_utils.time_function()\n",
    "models, predictions = time_function(ba_utils.load_cnn_models)(results_dirname, feature_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaded Model Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keys = set(models.keys())\n",
    "prediction_keys = set(predictions.keys())\n",
    "print(f\"{len(model_keys):7d} Models\")\n",
    "print(f\"{len(prediction_keys):7d} Predictions\")\n",
    "print()\n",
    "print(f\"Missing Models:      {len(prediction_keys-model_keys)}\")\n",
    "print(f\"Missing Predictions: {len(model_keys-prediction_keys)}\")\n",
    "print()\n",
    "for field in ModelKey._fields:\n",
    "    counts = Counter(getattr(model_key, field) for model_key in prediction_keys)\n",
    "    print(f\"{len(counts):6d} {field}\")\n",
    "    \n",
    "best_model_lookup = get_best_model_lookup(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "expected_performance_df = get_expected_performance(samples, target, predictions)\n",
    "with pd.option_context(\"display.max_rows\", 10, \"display.max_columns\", None):\n",
    "    display(expected_performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp_df = expected_performance_df.set_index([\n",
    "    \"variables\",\n",
    "    \"window\",\n",
    "    \"horizon\",\n",
    "    \"dim\",\n",
    "    \"training_filter\",\n",
    "    \"n_epochs\"\n",
    "]).sort_index()\n",
    "\n",
    "tmp_dirname = os.path.join(os.path.dirname(results_dirname), \"plots\")\n",
    "tmp_filename = os.path.join(tmp_dirname, \"performance.png\")\n",
    "ba_utils.mkdirs(tmp_dirname)\n",
    "ba_utils.display_bar_plot(\n",
    "    tmp_df[[\"mean\"]].transpose(),\n",
    "    tmp_df[[\"std\"]].rename(columns={\"std\":\"mean\"}).transpose(),\n",
    "    ax_title=\"Overall Performance\",\n",
    "    ncol=1,\n",
    "    filename=tmp_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance by `dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(ModelKey._fields)\n",
    "tmp_df = pd.pivot_table(\n",
    "    expected_performance_df,\n",
    "    values=\"mean\",\n",
    "    index=[\"variables\", \"window\", \"horizon\", \"training_filter\"],\n",
    "    columns=[\"dim\"],\n",
    "    aggfunc=np.sum\n",
    ").sort_index()\n",
    "tmp_yerr_df = pd.pivot_table(\n",
    "    expected_performance_df,\n",
    "    values=\"std\",\n",
    "    index=[\"variables\", \"window\", \"horizon\", \"training_filter\"],\n",
    "    columns=[\"dim\"],\n",
    "    aggfunc=np.sum\n",
    ").sort_index()\n",
    "\n",
    "tmp_dirname = os.path.join(os.path.dirname(results_dirname), \"plots/dim\")\n",
    "tmp_filename = os.path.join(tmp_dirname, \"performance.png\")\n",
    "ba_utils.mkdirs(tmp_dirname)\n",
    "ba_utils.display_bar_plot(\n",
    "    tmp_df,\n",
    "    tmp_yerr_df.rename(columns={\"std\":\"mean\"}),\n",
    "    ax_title=\"Performance by dim\",\n",
    "    label_rotation=25,\n",
    "    label_alignment=\"right\",\n",
    "    ncol=len(tmp_df.columns),\n",
    "    filename=tmp_filename\n",
    ")\n",
    "\n",
    "for sample,sample_df in samples.items():\n",
    "    display(HTML(f\"<h1>{sample}</h1>\"))\n",
    "    tmp_filename = os.path.join(\n",
    "        tmp_dirname,\n",
    "        \"-\".join(map(\n",
    "            lambda item: \"_\".join(item.split()),\n",
    "            sample\n",
    "        )) + \".png\"\n",
    "    )\n",
    "    print(tmp_filename)\n",
    "    display_model_groupings(\n",
    "        sample,\n",
    "        sample_df,\n",
    "        target,\n",
    "        predictions,\n",
    "        get_best_model_lookup(models),\n",
    "        [\"dim\"],\n",
    "        filename=tmp_filename\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance by `training_filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp_df = pd.pivot_table(\n",
    "    expected_performance_df,\n",
    "    values=\"mean\",\n",
    "    index=[\"variables\", \"window\", \"horizon\", \"dim\"],\n",
    "    columns=[\"training_filter\"],\n",
    "    aggfunc=np.sum\n",
    ")\n",
    "tmp_yerr_df = pd.pivot_table(\n",
    "    expected_performance_df,\n",
    "    values=\"std\",\n",
    "    index=[\"variables\", \"window\", \"horizon\", \"dim\"],\n",
    "    columns=[\"training_filter\"],\n",
    "    aggfunc=np.sum\n",
    ")\n",
    "\n",
    "tmp_dirname = os.path.join(os.path.dirname(results_dirname), \"plots/training_filter\")\n",
    "tmp_filename = os.path.join(tmp_dirname, \"performance.png\")\n",
    "ba_utils.mkdirs(tmp_dirname)\n",
    "ba_utils.display_bar_plot(\n",
    "    tmp_df,\n",
    "    tmp_yerr_df.rename(columns={\"std\":\"mean\"}),\n",
    "    ax_title=\"Performance by training_filter\",\n",
    "    label_rotation=25,\n",
    "    label_alignment=\"right\",\n",
    "    ncol=len(tmp_df.columns),\n",
    "    filename=tmp_filename\n",
    ")\n",
    "\n",
    "for sample,sample_df in samples.items():\n",
    "    display(HTML(f\"<h1>{sample}</h1>\"))\n",
    "    tmp_filename = os.path.join(\n",
    "        tmp_dirname,\n",
    "        \"-\".join(map(\n",
    "            lambda item: \"_\".join(item.split()),\n",
    "            sample\n",
    "        )) + \".png\"\n",
    "    )\n",
    "    print(tmp_filename)\n",
    "    display_model_groupings(\n",
    "        sample,\n",
    "        sample_df,\n",
    "        target,\n",
    "        predictions,\n",
    "        get_best_model_lookup(models),\n",
    "        [\"training_filter\"],\n",
    "        filename=tmp_filename\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance by `variables`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp_df = pd.pivot_table(\n",
    "    expected_performance_df,\n",
    "    values=\"mean\",\n",
    "    index=[\"window\", \"horizon\", \"dim\", \"training_filter\"],\n",
    "    columns=[\"variables\"],\n",
    "    aggfunc=np.sum\n",
    ").sort_index()\n",
    "tmp_yerr_df = pd.pivot_table(\n",
    "    expected_performance_df,\n",
    "    values=\"std\",\n",
    "    index=[\"window\", \"horizon\", \"dim\", \"training_filter\"],\n",
    "    columns=[\"variables\"],\n",
    "    aggfunc=np.sum\n",
    ").sort_index()\n",
    "\n",
    "tmp_dirname = os.path.join(os.path.dirname(results_dirname), \"plots/variables\")\n",
    "tmp_filename = os.path.join(tmp_dirname, \"performance.png\")\n",
    "ba_utils.mkdirs(tmp_dirname)\n",
    "ba_utils.display_bar_plot(\n",
    "    tmp_df,\n",
    "    tmp_yerr_df.rename(columns={\"std\":\"mean\"}),\n",
    "    ax_title=\"Performance by variables\",\n",
    "    label_rotation=25,\n",
    "    label_alignment=\"right\",\n",
    "    ncol=1,\n",
    "    filename=tmp_filename\n",
    ")\n",
    "\n",
    "for sample,sample_df in samples.items():\n",
    "    display(HTML(f\"<h1>{sample}</h1>\"))\n",
    "    tmp_filename = os.path.join(\n",
    "        tmp_dirname,\n",
    "        \"-\".join(map(\n",
    "            lambda item: \"_\".join(item.split()),\n",
    "            sample\n",
    "        )) + \".png\"\n",
    "    )\n",
    "    print(tmp_filename)\n",
    "    display_model_groupings(\n",
    "        sample,\n",
    "        sample_df,\n",
    "        target,\n",
    "        predictions,\n",
    "        get_best_model_lookup(models),\n",
    "        [\"variables\"],\n",
    "        filename=tmp_filename\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_video(sample, sample_df, target, model_keys, grouping_fileds, predictions):\n",
    "    assert all(model_keys[0].variables == model_key.variables for model_key in model_keys)\n",
    "    assert all(model_keys[0].window == model_key.window for model_key in model_keys)\n",
    "    assert all(model_keys[0].horizon == model_key.horizon for model_key in model_keys)\n",
    "    \n",
    "    features = list(model_keys[0].variables)\n",
    "    window = model_keys[0].window\n",
    "    horizon = model_keys[0].horizon\n",
    "    colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "    \n",
    "    lines = list()\n",
    "    points = list()\n",
    "    spans = list()\n",
    "    \n",
    "    n_cols = len(features)\n",
    "    n_rows = 3\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(16,10))\n",
    "    gs = mpl.gridspec.GridSpec(\n",
    "        nrows=n_rows,\n",
    "        ncols=n_cols,\n",
    "        figure=fig\n",
    "    )\n",
    "    \n",
    "    ax = fig.add_subplot(gs[:-1,:])\n",
    "    target_line, = ax.plot(sample_df.loc[:,pd.IndexSlice[:,:,target]], color=\"black\", label=target)\n",
    "    for ii,model_key in enumerate(model_keys):\n",
    "        color = [\"r\", \"g\", \"b\"][ii % 3]\n",
    "        values = predictions[model_key][sample]\n",
    "        label = \"; \".join([\n",
    "            f\"{field}={getattr(model_key, field)}\"\n",
    "            for field in grouping_fileds\n",
    "        ])\n",
    "        lines.append(\n",
    "            ax.plot(sample_df.index[-len(values):], values, color=color)[0]\n",
    "        )\n",
    "        points.append(\n",
    "            ax.scatter(sample_df.index[-len(values)], values[0], color=color, label=label)\n",
    "        )\n",
    "    spans.append(\n",
    "        ax.axvspan(sample_df.index[0], sample_df.index[window], color=\"gray\", alpha=0.25)\n",
    "    )\n",
    "    handles, labels = ax.get_legend_handles_labels() \n",
    "    title = \"\\n\".join([\n",
    "        f\"{field}={getattr(model_keys[0],field)}\"\n",
    "        for field in model_keys[0]._fields\n",
    "        if field not in grouping_fileds\n",
    "    ])\n",
    "    ax.set_title(title, loc=\"left\")\n",
    "    \n",
    "    for ii,feature in enumerate(features):\n",
    "        color = colors[ii % len(colors)]\n",
    "        ax = fig.add_subplot(gs[-1,ii])\n",
    "        ax.plot(sample_df.loc[:,pd.IndexSlice[:,:,feature]], color=color)\n",
    "        spans.append(\n",
    "            ax.axvspan(sample_df.index[0], sample_df.index[window], color=color, alpha=0.25)\n",
    "        )\n",
    "        ax.set_title(feature)\n",
    "    fig.legend(handles, labels, loc=\"upper center\", bbox_to_anchor=(0.5, -0.07), ncol=1)\n",
    "    plt.show()\n",
    "#     return\n",
    "    \n",
    "    def init():\n",
    "        for line in lines:\n",
    "            line.set_data([], [])\n",
    "        for point in points:\n",
    "            point.set_offsets(np.empty(shape=(0,2)))\n",
    "        for span in spans:\n",
    "            xy = span.get_xy()\n",
    "            xy[[0,1,4],0] = sample_df.index[0]\n",
    "            xy[[2,3],0] = sample_df.index[window]\n",
    "            span.set_xy(xy)\n",
    "        legend = fig.legend(handles, labels, loc=\"upper center\", bbox_to_anchor=(0.5, -0.07), ncol=1)\n",
    "        return lines + points + spans + [legend]\n",
    "\n",
    "    def animate(frame):\n",
    "        print(frame, end=\" \")\n",
    "        \n",
    "        target_line.set_data(\n",
    "            sample_df.index[:frame+window],\n",
    "            sample_df.iloc[:frame+window,:].loc[:,pd.IndexSlice[:,:,target]]\n",
    "        )\n",
    "        for line,point,model_key in zip(lines,points,model_keys):\n",
    "            values = predictions[model_key][sample]\n",
    "            assert len(values) == len(sample_df.index)\n",
    "            line.set_data(sample_df.index[:frame+window+horizon], values[:frame+window+horizon])\n",
    "            point.set_offsets([[sample_df.index[frame+window+horizon-1], values[frame+window+horizon-1]]])\n",
    "        for span in spans:\n",
    "            xy = span.get_xy()\n",
    "            xy[[0,1,4],0] = sample_df.index[frame]\n",
    "            xy[[2,3],0] = sample_df.index[frame+window]\n",
    "            span.set_xy(xy)\n",
    "        legend = fig.legend(handles, labels, loc=\"upper center\", bbox_to_anchor=(0.5, -0.07), ncol=1)\n",
    "        return [target_line] + lines + points + spans + [legend]\n",
    "    \n",
    "    #frames = np.arange(len(sample_df.index)-window-horizon+1)\n",
    "    argmax = np.argmax(sample_df.loc[:,pd.IndexSlice[:,:,target]].values)\n",
    "    frames= np.arange(argmax-5*window, argmax+2*window)\n",
    "    frames = frames[::int(np.ceil(len(frames)/60))]\n",
    "    print(len(frames))\n",
    "    anim = mpl.animation.FuncAnimation(\n",
    "        fig,\n",
    "        animate,\n",
    "        init_func=init,\n",
    "        frames=frames,\n",
    "        interval=100,\n",
    "        blit=True,\n",
    "    )\n",
    "#     anim.save(\"test.gif\", writer=\"imagemagick\")\n",
    "    display(HTML(anim.to_jshtml()))\n",
    "\n",
    "grouping_fileds = [\"training_filter\"]\n",
    "model_groupings = get_model_groupings(best_model_lookup.keys(), grouping_fileds)\n",
    "for ii,(sample,sample_df) in enumerate(samples.items()):\n",
    "    display(HTML(f\"<h1>{sample}</h1>\"))\n",
    "    sample_model_groupings = {\n",
    "        model_key:list(map(best_model_lookup.get,model_grouping))\n",
    "        for model_key,model_grouping in model_groupings.items()\n",
    "        if model_key.holdouts[0] == sample\n",
    "    }\n",
    "    for general_model_key,specific_model_keys in sample_model_groupings.items():\n",
    "        make_video(sample, sample_df, target, specific_model_keys, grouping_fileds, predictions)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
