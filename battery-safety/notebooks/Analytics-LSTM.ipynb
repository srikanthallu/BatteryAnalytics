{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal\n",
    "import scipy.signal\n",
    "import sklearn.isotonic\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "try:\n",
    "    import batteryanalytics\n",
    "except ModuleNotFoundError as ie:\n",
    "    sys.path.append( os.path.join(os.path.abspath(\"\"), \"../\") )\n",
    "    import batteryanalytics\n",
    "from batteryanalytics import utils as ba_utils\n",
    "from batteryanalytics.nn import LSTM\n",
    "\n",
    "from IPython.core.display import display, HTML, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basename = \"mechanical_loading_data.csv.gz\"\n",
    "dirname = \"../data\"\n",
    "\n",
    "window = 150\n",
    "horizon = 300\n",
    "\n",
    "# features = [\"2000 Pounds [Pounds]\", \"Voltage [V]\", \"Temperature [C]\"]\n",
    "# features = [\"2000 Pounds [Pounds]\", \"Voltage [V]\"]\n",
    "features = [\n",
    "    \"2000 Pounds [Pounds]\", \"Voltage [V]\",\n",
    "    \"2000 Pounds [Pounds] (gradient 1)\", \"Voltage [V] (gradient 1)\",\n",
    "]\n",
    "target = \"Temperature [C]\"\n",
    "\n",
    "n_epochs = 5\n",
    "training_filters = [\"none\", \"savgol\", \"isotonic_regression\"]\n",
    "hidden_dims = [2,4,8,16]\n",
    "n_models = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suppress_stdout(func):\n",
    "    import functools\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        from contextlib import redirect_stdout\n",
    "        with open(os.devnull, \"w\") as fp:\n",
    "            with redirect_stdout(fp):\n",
    "                value = func(*args, **kwargs)\n",
    "        return value\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join(dirname, basename)\n",
    "raw_df = pd.read_csv(filename, header=[0,1,2], index_col=0, compression=\"gzip\")\n",
    "raw_df.info(verbose=True)\n",
    "with pd.option_context(\"display.max_rows\", 10, \"display.max_columns\", None):\n",
    "    display(raw_df)\n",
    "    display(raw_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_df = raw_df.copy()\n",
    "\n",
    "levels_0 = list()\n",
    "levels_1 = list()\n",
    "for column in data_df.columns:\n",
    "    if column[0] not in levels_0:\n",
    "        levels_0.append(column[0])\n",
    "    if column[1] not in levels_1:\n",
    "        levels_1.append(column[1])\n",
    "\n",
    "samples = {\n",
    "    pair:data_df.xs(pair, axis=\"columns\", level=(0,1), drop_level=False).dropna()\n",
    "    for pair in itertools.product(levels_0, levels_1)\n",
    "}\n",
    "summary1_df = pd.DataFrame(\n",
    "    [[\n",
    "            sample_df.index[-1]-sample_df.index[0],\n",
    "            len(sample_df.index)\n",
    "    ] for sample_df in samples.values() ],\n",
    "    columns=[\"time span\", \"samples\"]\n",
    ").describe()\n",
    "\n",
    "for sample_df in samples.values():\n",
    "    series = sample_df.loc[:,pd.IndexSlice[:,:,target]].copy()\n",
    "    idx = np.squeeze(series.values).argmax()\n",
    "    sample_df.drop(sample_df.index[2*idx+1:], inplace=True)\n",
    "    # vmin, vmax = series.min(), series.max()\n",
    "    # delta = vmax - vmin\n",
    "    # idx = series > 0.05*delta + vmin\n",
    "    # cut_off = np.squeeze(idx.iloc[::-1].idxmax().values).item()\n",
    "    # sample_df.drop(sample_df.index[sample_df.index > cut_off], inplace=True)\n",
    "\n",
    "summary2_df = pd.DataFrame(\n",
    "    [[\n",
    "            sample_df.index[-1]-sample_df.index[0],\n",
    "            len(sample_df.index)\n",
    "    ] for sample_df in samples.values() ],\n",
    "    columns=[\"time span\", \"samples\"]\n",
    ").describe()\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", 10, \"display.max_columns\", None):\n",
    "    display(summary1_df)\n",
    "    display(summary2_df)\n",
    "    display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_unique(items):\n",
    "    seen = set()\n",
    "    unique = list()\n",
    "    for item in items:\n",
    "        if item not in seen:\n",
    "            unique.append(item)\n",
    "            seen.add(item)\n",
    "    return unique        \n",
    "level0 = get_unique(raw_df.columns.get_level_values(0))\n",
    "level1 = get_unique(raw_df.columns.get_level_values(1))\n",
    "level2 = get_unique(raw_df.columns.get_level_values(2))\n",
    "\n",
    "for c0 in level0:\n",
    "    display(HTML(f\"<h1>{c0}</h1>\"))\n",
    "    \n",
    "    if not c0.startswith(\"500\"):\n",
    "        continue\n",
    "    for c1 in level1:\n",
    "        display(HTML(f\"<h2>{c1}</h2>\"))\n",
    "        if not c1.startswith(\"20\"):\n",
    "            continue\n",
    "        sample_df = samples[(c0,c1)]\n",
    "        \n",
    "        n_cols = 3\n",
    "        n_rows = int(np.ceil(len(level2)/n_cols))\n",
    "#         fig = plt.figure(constrained_layout=True, figsize=(n_cols*8,5*n_rows))\n",
    "        fig = plt.figure(constrained_layout=True, figsize=(16,8))\n",
    "        gs = mpl.gridspec.GridSpec(\n",
    "            nrows=n_rows,\n",
    "            ncols=n_cols,\n",
    "            figure=fig\n",
    "        )\n",
    "        for ii,c2 in enumerate(level2):\n",
    "            ax = fig.add_subplot(gs[ii])\n",
    "            ax.plot(sample_df[(c0,c1,c2)], zorder=1)\n",
    "            ax.plot(raw_df[(c0,c1,c2)], zorder=0)\n",
    "            ax.set_xlim(raw_df.index[0], raw_df.index[-1])\n",
    "            ax.set_title(c2)\n",
    "            ax.set_xlabel(\"Time\")\n",
    "        fig.suptitle(\"{}, {}\".format(c0,c1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(samples, n_split, features, target, window, horizon, hidden_dim, n_epochs, training_filter, model_id):\n",
    "    model = LSTM(\n",
    "        window=window,\n",
    "        horizon=horizon,\n",
    "        hidden_dim=hidden_dim,\n",
    "        n_epochs=n_epochs,\n",
    "        batch_learning=False,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        compute_device=\"cpu\",\n",
    "        parallel=False,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    samples_items = np.empty(shape=len(samples), dtype=object)\n",
    "    samples_items[:] = list(samples.items())\n",
    "\n",
    "    splitter = sklearn.model_selection.LeaveOneOut()\n",
    "    splits = list(splitter.split(samples_items))\n",
    "    train_idx,test_idx = splits[n_split]\n",
    "\n",
    "    XX = [\n",
    "        value.loc[:,pd.IndexSlice[:,:,features]].values\n",
    "        for key,value in samples_items[train_idx] \n",
    "    ]\n",
    "    yy = [\n",
    "        value.loc[:,pd.IndexSlice[:,:,target]].values\n",
    "        for key,value in samples_items[train_idx] \n",
    "    ]\n",
    "    if training_filter == \"savgol\":\n",
    "        filter_function = lambda x:scipy.signal.savgol_filter(\n",
    "            x,\n",
    "            window_length=99,\n",
    "            polyorder=2\n",
    "        )\n",
    "    elif training_filter == \"isotonic_regression\":\n",
    "        filter_function = lambda x:np.hstack([\n",
    "            sklearn.isotonic.isotonic_regression(x[:np.argmax(x)], increasing=True) if np.argmax(x) > 0 else [],\n",
    "            sklearn.isotonic.isotonic_regression(x[np.argmax(x):], increasing=False) if np.argmax(x) < len(x) else []\n",
    "        ])\n",
    "    elif training_filter == \"none\":\n",
    "        filter_function = None\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected training_filter value ({filter_function})\")\n",
    "\n",
    "    model.fit(XX, yy, filter_function=filter_function)\n",
    "\n",
    "    filename = os.path.join(\n",
    "        ba_utils.get_lstm_dirname([key for key,value in samples_items[test_idx]]),\n",
    "        ba_utils.get_lstm_basename(features, window, horizon, model.hidden_dim, model.n_epochs, training_filter, model_id)\n",
    "    ).replace(\"joblib\", \"npz\")\n",
    "\n",
    "    keys = list(map(ba_utils.sanitize_holdout_name, samples.keys()))\n",
    "    values = list(map(\n",
    "        np.squeeze,\n",
    "        model.transform(\n",
    "            list(map(\n",
    "                lambda df:df.loc[:,pd.IndexSlice[:,:,features]].values,\n",
    "                samples.values()\n",
    "            ))\n",
    "        )\n",
    "    ))\n",
    "    kwargs = dict(zip(keys,values))\n",
    "\n",
    "    ba_utils.mkdirs(os.path.dirname(filename))\n",
    "    np.savez(filename, **kwargs)\n",
    "\n",
    "    filename = os.path.join(\n",
    "        ba_utils.get_lstm_dirname([key for key,value in samples_items[test_idx]]),\n",
    "        ba_utils.get_lstm_basename(features, model.window, model.horizon, model.hidden_dim, model.n_epochs, training_filter, model_id)\n",
    "    )\n",
    "    filename = ba_utils.save_model(filename, model)\n",
    "    return model, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model_id in range(1,n_models+1):\n",
    "    for training_filter in training_filters:\n",
    "        for hidden_dim in hidden_dims:\n",
    "            for n_split in range(len(samples)):\n",
    "                model,_ = train_model(\n",
    "                    samples=samples,\n",
    "                    n_split=n_split,\n",
    "                    features=features,\n",
    "                    target=target,\n",
    "                    window=window,\n",
    "                    horizon=horizon,\n",
    "                    hidden_dim=hidden_dim,\n",
    "                    n_epochs=n_epochs,\n",
    "                    training_filter=training_filter,\n",
    "                    model_id=model_id\n",
    "                )\n",
    "                print(model._model)\n",
    "                ba_utils.display_loss_curve(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
